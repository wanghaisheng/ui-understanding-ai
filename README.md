# ui-understanding-ai

**how  agent work with user interface like human **

Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs

AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding

Generating Automatic Feedback on UI Mockups with Large Language Models

## Computer User Interface Understanding. A New Dataset and a Learning Framework

这篇论文《Computer User Interface Understanding: A New Dataset and a Learning Framework》由Andrés Muñoz和Daniel Borrajo撰写，主要探讨了计算机用户界面（UI）理解的问题。以下是论文的核心内容概述：

### 标题
- Computer User Interface Understanding: A New Dataset and a Learning Framework

### 作者
- Andrés Muñoz, Daniel Borrajo (J.P. Morgan AI Research)

### 摘要
- 论文提出了计算机UI理解的新任务，包括创建数据集和学习框架。
- 介绍了一个由视频组成的数据集，视频展示了用户执行一系列动作时桌面内容的变化。
- 提出了一个包含合成样本生成管道和对比学习方法的框架，利用图像特征的自然条件和树状关系来提高表示学习。

### 关键词
- UI Understanding
- Contrastive Learning
- Dataset

### 1. 引言
- 论文指出，尽管UI理解在移动应用和浏览器中已有研究，但完整计算机UI的表示学习尚未被探索。

### 2. 计算机用户界面理解
- 论文定义了描述计算状态的语言，包括用户交互的应用程序、视图和交互上下文。

### 3. 相关工作
- 讨论了UI检索、生成和数字设备控制方面的相关工作，以及对比学习在不同任务中的应用。

### 4. 数据集
- 介绍了新创建的DataVisualWorkflow数据集，包含14345帧，涉及多种软件应用和用户交互。

### 5. UI多任务对比学习（UIMTCon）
- 提出了一种新的半监督框架，通过合成样本生成器和对比学习方法来学习标记和未标记特征。

### 6. 实验
- 在DataVisualWorkflow数据集上评估模型，使用不同的度量标准，如AMI、Precision@1、R-Precision和mAP@R。

### 7. 结论
- UIMTCon框架在半监督计算机UI理解方面表现良好，为未来的UI表示学习研究提供了新平台。

### 致谢
- 论文由J.P. Morgan AI Research团队准备，不构成投资研究或建议。

### 参考文献
- 列出了与UI理解、对比学习和其他相关领域研究相关的参考文献。

论文的主要贡献包括提出了一个新的UI理解任务，开发了一个新的数据集，以及提出了一个新的学习框架，这些对于自动化企业工作流程的研究具有重要意义。



在这篇论文中，对比学习方法被应用于UI理解任务，主要通过以下几个步骤实现：

1. **数据集创建**:
   - 首先，作者创建了一个名为DataVisualWorkflow的新数据集，包含用户在计算机上执行操作的视频帧。这些帧展示了桌面内容的变化，包括不同的软件应用、视图和用户交互的上下文。

2. **特征表示学习**:
   - 利用合成样本生成管道和对比学习方法来学习图像的特征表示。这种方法利用了图像特征的自然条件和树状关系，通过多个部分任务同时进行来规范表示学习。

3. **合成样本生成**:
   - 为了增强数据集并处理未标记数据，作者提出了一个合成样本生成器，用于模拟上下文菜单和选中文本的存在。这些合成样本被用来模拟真实图像中可能缺失的标签。

4. **多任务对比学习框架（UIMTCon）**:
   - 论文提出了UIMTCon，一个新颖的半监督学习框架，用于同时学习标记和未标记的特征。该框架包括两个主要模块：合成样本生成器和嵌入网络。

5. **嵌入网络**:
   - 嵌入网络负责从合成样本和真实样本中提取特征表示，这些特征表示可以用于提取标记和未标记的特征。

6. **对比损失函数**:
   - 使用对比损失函数来优化学习过程，使得同一类别的图像在特征空间中更接近，而不同类别的图像更远离。

7. **层次化标签结构**:
   - 利用数据集中的层次化标签结构（如软件、视图、上下文），作者提出了一种分割层次损失（Split Hierarchy Loss, SHL）方法，它结合了不同层次的对比损失。

8. **实验评估**:
   - 在DataVisualWorkflow数据集上进行实验，使用多种度量标准（如AMI、Precision@1、R-Precision和mAP@R）来评估学习到的特征表示的质量。

通过这种方法，对比学习方法在UI理解任务中能够有效地捕捉到用户界面的不同状态，并提高对这些状态进行分类和识别的准确性。论文的实验结果表明，所提出的框架在多个指标上优于基线方法，展示了对比学习方法在UI理解任务中的有效性。


这篇论文中提到的新数据集DataVisualWorkflow是通过以下步骤收集和整理的：

1. **数据采集**:
   - 数据集包含来自两个不同来源的数据：模拟工作流（mock-up workflows）和YouTube视频教程。使用OBS Studio的屏幕录制器录制模拟工作流，这些工作流由单一人员在一个月内以每秒25帧（FPS）和1920×1080的分辨率录制。

2. **视频选择**:
   - 从YouTube下载的视频教程遵循的约束是，视频中的长形式动作应该在与数据集中相同的软件应用程序中完成。

3. **数据清洗**:
   - 计算机工作流和教程中包含许多重复帧和几乎没有相关信息的帧。使用运动检测算法过滤掉变化很小的帧。这个算法基于先前工作的修改版本，通过高斯模糊、膨胀操作和二值化处理来识别和保存具有显著变化的帧。

4. **数据标注**:
   - 在录制时，使用xdotool记录屏幕上的软件应用名称和视图。由于这类工具可能存在同步问题、名称不标准化或返回空字符串等问题，因此进行了彻底的逐帧质量保证过程，并手动添加了上下文标签。

5. **上下文标签**:
   - 仅在测试集和YouTube集上添加了上下文标签，包括上下文菜单、选中的文本和无上下文。

6. **数据集划分**:
   - 录制的视频被分为训练集和测试集。训练集包含9597帧，而测试集包含4297帧。测试集包含软件、视图和上下文的标签，而在训练集上仅标注了软件和视图。

7. **数据集特点**:
   - DataVisualWorkflow数据集包括27个描述软件和视图的类别，以及多达81个描述软件、视图和上下文组合的类别。数据集还包含上下文菜单和选中文本的识别标签。

8. **数据集应用**:
   - 论文中提到，DataVisualWorkflow数据集用于半监督表示学习。然而，在未来，该数据集也可能对无监督表示学习、动作识别、从演示学习、开放集学习以及异常行为检测等领域的研究有用。

通过这些步骤，作者们创建了一个能够支持计算机UI理解任务的数据集，它包含了丰富的计算机软件应用和用户交互的实例。这个数据集为研究者提供了一个平台，以进一步探索UI表示学习的不同方法。
DataVisualWorkflow数据集在实际应用中有多个潜在的应用场景，主要包括：

1. **工作流自动化**:
   - 通过观察用户在计算机上执行的任务，可以开发系统来自动化企业环境中的工作流程。例如，自动化电子邮件处理、数据输入和文档编辑等任务。

2. **用户交互识别**:
   - 利用数据集中的用户界面和用户交互信息，可以开发更自然和高效的计算机控制方法，如上下文感知的交互识别。

3. **用户中心的AI辅助**:
   - 开发AI模型，通过预测用户行为和在用户界面中提供推荐，来辅助用户完成特定任务。

4. **任务自动化**:
   - 创建能够无缝切换不同软件应用程序并执行复杂认知功能的自动化控制系统。

5. **无监督和半监督学习**:
   - 利用数据集进行无监督和半监督学习，以解决缺乏标签数据的问题，提高模型在实际应用中的泛化能力。

6. **动作识别**:
   - 通过分析用户在计算机界面上的操作序列，可以识别用户的行为模式，用于个性化推荐或用户行为分析。

7. **学习从演示**:
   - 使用数据集中的长形式动作演示来训练模型，使其能够模仿和学习用户的特定操作。

8. **开放集学习**:
   - 利用数据集进行开放集学习，使模型能够处理在训练时未见过的新的软件应用或用户界面布局。

9. **异常行为检测**:
   - 通过分析用户界面上的变化，可以检测和识别不符合常规操作的异常行为。

10. **计算机视觉和机器学习研究**:
    - 提供一个丰富的资源，用于计算机视觉和机器学习领域的研究，如图像识别、对象检测、图像分割等。

11. **用户界面设计和评估**:
    - 利用数据集中的用户交互信息，可以帮助设计师评估和改进用户界面的设计。

DataVisualWorkflow数据集的多样性和丰富性使其成为研究和开发与计算机用户界面相关的自动化系统和智能应用的宝贵资源。

DataVisualWorkflow数据集可以通过以下方式帮助提升用户界面设计中的用户体验：

1. **用户行为分析**:
   - 数据集提供了用户与计算机界面交互的视频帧，这些信息可以用来分析用户的行为模式和偏好，从而设计更符合用户习惯的界面。

2. **工作流程优化**:
   - 通过观察用户在完成特定任务时的操作序列，可以识别出低效或复杂的步骤，并对其进行优化以简化用户的工作流程。

3. **交互设计改进**:
   - 数据集可以帮助设计师理解用户如何与不同的UI元素交互，例如按钮、菜单和输入框等，从而改进这些元素的布局、大小和响应性。

4. **上下文感知设计**:
   - 数据集中的上下文信息（如选中的文本和打开的上下文菜单）可以用来开发上下文感知的UI，提供更加个性化和情境相关的功能。

5. **自动化测试和评估**:
   - 利用数据集中的交互记录，可以自动化测试用户界面的可用性，快速识别问题区域并进行迭代改进。

6. **个性化用户体验**:
   - 通过分析用户如何使用不同的软件应用，设计师可以为不同的用户群体定制个性化的UI设计。

7. **辅助功能开发**:
   - 数据集可以揭示用户在与UI交互时可能遇到的困难，从而激发辅助功能的开发，帮助有特殊需求的用户更好地使用应用。

8. **设计模式和最佳实践的发现**:
   - 通过大规模的用户界面分析，可以发现有效的设计模式和最佳实践，并将其应用于新的界面设计中。

9. **多模态交互探索**:
   - 数据集包含的多种软件应用和用户交互可以激发多模态交互方式的探索，结合视觉、触觉和声音等多种感官体验。

10. **用户反馈和市场研究**:
    - 数据集可以作为用户反馈的资源，帮助设计师理解用户需求和市场趋势，从而设计出更具吸引力和实用性的界面。

11. **教育和培训材料**:
    - 数据集可以作为教育工具，帮助新用户学习如何更有效地使用软件应用，通过观察经验丰富的用户的交互模式。

通过这些方式，DataVisualWorkflow数据集为用户界面设计提供了丰富的实际使用情境和用户反馈，有助于设计师创建更加直观、高效和愉悦的用户体验。

DataVisualWorkflow数据集在自动化工作流程中的应用案例可以非常多样，以下是一些具体的应用实例：

1. **电子邮件处理自动化**:
   - 通过分析用户如何在电子邮件客户端中选择、复制和粘贴信息，可以创建宏或脚本来自动化日常的邮件处理任务。

2. **数据输入和迁移**:
   - 观察用户如何在电子表格和数据库应用程序之间复制和粘贴数据，自动化这些重复性的数据迁移任务。

3. **文档编辑和格式化**:
   - 利用数据集中的文档编辑操作，如格式设置、页眉页脚添加和文档导出，自动化文档的准备和格式化过程。

4. **Web内容抓取**:
   - 分析用户如何在浏览器中搜索信息和填写表单，自动化从Web页面抓取数据或提交信息的任务。

5. **报告生成**:
   - 通过识别用户在不同应用程序中汇总和生成报告的步骤，自动化报告的创建过程。

6. **任务调度和提醒**:
   - 观察用户在日历和任务管理工具中的操作，自动化任务的调度和提醒设置。

7. **客户关系管理（CRM）自动化**:
   - 分析用户如何在CRM系统中记录客户互动和更新信息，自动化客户数据的录入和同步。

8. **内容创建和管理**:
   - 使用数据集中的内容创建操作，如图像编辑和布局设计，自动化营销材料或社交媒体帖子的制作。

9. **财务数据处理**:
   - 观察用户在财务软件中的操作，自动化发票处理、支付记录和账目核对等财务任务。

10. **项目跟踪和时间记录**:
    - 分析用户在项目管理工具中的操作，自动化跟踪项目进度和记录工作时间。

11. **自动化错误检测和修复**:
    - 利用数据集中的用户交互模式，开发自动化脚本检测和修复常见的用户界面错误。

12. **自定义宏和快捷操作**:
    - 观察用户常用的操作序列，创建自定义宏或快捷键来加速日常任务的执行。

13. **智能助手开发**:
    - 利用用户交互数据训练机器学习模型，开发能够预测用户需求并提供相应操作建议的智能助手。

14. **用户界面测试自动化**:
    - 使用数据集中的交互记录作为测试脚本，自动化用户界面的测试过程，确保UI的稳定性和可用性。

15. **业务流程优化分析**:
    - 通过分析整个工作流程的数据，识别瓶颈和效率低下的环节，为业务流程优化提供决策支持。

这些应用案例展示了 DataVisualWorkflow 数据集如何支持开发人员和业务分析师理解和自动化复杂的工作流程，提高效率并减少人为错误。



## ScreenAI: A Vision-Language Model for UI and Infographics Understanding

这篇论文《ScreenAI: A Vision-Language Model for UI and Infographics Understanding》由Gilles Baechler等人撰写，介绍了一个名为ScreenAI的视觉-语言模型，专门用于理解和处理用户界面（UI）和信息图表。以下是论文的核心内容概述：

### 标题
- ScreenAI: A Vision-Language Model for UI and Infographics Understanding

### 作者
- Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen, Abhanshu Sharma (Google DeepMind)

### 摘要
- ScreenAI模型通过结合视觉和语言模型，专注于UI和信息图表的理解，包括问题回答、元素注释、总结、导航等任务。
- 模型结构基于PaLI架构，并采用pix2struct的灵活图像块策略，通过独特数据集混合进行训练。

### 主要贡献
- 提出了ScreenAI，一个综合性的视觉-语言模型，利用UI和信息图表的共同视觉语言和设计复杂性。
- 引入了UI的文本表示方法，用于在预训练阶段教授模型如何理解UI。
- 利用这种新的UI表示和大型语言模型（LLMs）自动大规模生成训练数据。
- 定义了预训练和微调混合任务，涵盖UI和信息图表理解的广泛任务范围。
- 发布了三个评估数据集，用于屏幕注释、简短屏幕问题回答和复杂屏幕问题回答。

### 相关工作
- 论文回顾了与屏幕基础UI模型、通用基础模型和高效的视觉-语言模型相关的研究。

### 方法论
- 论文描述了ScreenAI模型的架构，包括视觉编码器、多模态编码器和自回归解码器。
- 讨论了模型配置、训练阶段和自动数据生成过程。

### 自动数据生成
- 论文详细介绍了如何利用小型专业模型自动生成和标注数据，包括屏幕注释、问题回答、导航和总结任务。

### 数据混合
- 定义了预训练和微调任务的不同数据集，包括屏幕注释、问题回答、导航和总结。

### 实验和结果
- 展示了ScreenAI模型在多种屏幕和信息图表相关任务上的性能，并与现有技术进行了比较。
- 分析了模型大小对整体性能的影响，并进行了消融研究来验证模型设计选择。

### 结论
- ScreenAI模型通过统一的表示和自监督学习任务的混合训练，在屏幕相关任务和信息图表任务上取得了积极的性能转移。

### 致谢和贡献声明
- 对项目团队成员和其他贡献者表示感谢，并声明了论文的主要作者和项目负责人。

论文的核心贡献在于提出了ScreenAI模型，该模型通过结合视觉和语言理解，提高了UI和信息图表理解任务的性能，并在多个公共基准测试中取得了新的最佳结果。此外，论文还发布了三个新的数据集，以促进未来在屏幕相关问题回答方面的研究。

在这篇论文中，自动数据生成技术是ScreenAI模型预训练阶段的关键部分，它允许模型从大量且多样化的数据中学习，而无需手动进行昂贵的注释工作。以下是自动数据生成技术的工作流程：

1. **屏幕注释（Screen Annotation）**:
   - 通过使用各种模型（如DETR检测模型、图标分类器、OCR引擎等）自动注释屏幕截图，生成包含UI元素名称、文本内容、元素描述和边界框坐标的详细标签。

2. **屏幕架构生成**:
   - 利用OCR和图像字幕模型，结合图标分类和其他屏幕元素，创建屏幕的详细和整体描述。

3. **大型语言模型（LLMs）**:
   - 使用大型语言模型（如PaLM 2-S）根据屏幕架构生成问题-答案（QA）对。这包括两个阶段：首先生成屏幕架构，然后使用该架构生成合成数据。

4. **任务生成流水线**:
   - 利用小型专业模型生成和标注数据，然后使用LLMs生成与屏幕相关的任务，如QA、导航和总结任务。

5. **数据验证**:
   - 可选地，使用另一个LLM或人类评估者对生成的数据进行验证，确保数据质量。

6. **问题-答案对生成**:
   - 使用LLMs根据屏幕架构和预定义的提示生成问题和短答案列表。这些答案需要尽可能简短，仅包含必要信息。

7. **多步骤导航指令**:
   - 对于屏幕导航任务，生成基于屏幕架构的单步导航指令和相应的答案，例如点击某个元素的坐标。

8. **屏幕摘要**:
   - 生成屏幕内容的简短摘要，摘要应聚焦于内容而非具体命名各种UI元素。

9. **数据多样性**:
   - 通过LLMs生成多样化且真实的任务，增强预训练数据集的深度和广度。

10. **数据集权重**:
    - 在预训练混合中，根据数据集的大小按比例加权，确保模型在多样化的任务上进行训练。

这种自动数据生成方法不仅提高了数据的多样性和复杂性，而且通过模拟广泛的用户交互和场景，显著增强了模型对用户界面的理解能力。此外，这种方法的可扩展性和效率使得模型能够处理大规模数据集，为构建通用的视觉-语言模型提供了强大的支持。
是的，论文中提到了发布了三个新的评估数据集，这些数据集专门用于评估ScreenAI模型在不同任务上的性能。以下是每个数据集的简要描述：

1. **屏幕注释（Screen Annotation, SA）**:
   - 这个数据集旨在评估模型对用户界面元素布局注释和空间理解的能力。它由来自Rico数据集的4.2K截图组成，每个UI元素都由人工标注者进行了注释，包括边界框和UI类别。

2. **简短屏幕问题回答（ScreenQA Short, SQA Short）**:
   - 为了与其它问题回答任务的输出格式保持一致，研究者创建了这个数据集，它包含了一系列的问题和答案对。对于ScreenQA数据集中的每个问题，研究者生成了一个新的答案列表，这些答案是问题的简短答案。使用最大F1分数作为评估指标。

3. **复杂屏幕问题回答（Complex ScreenQA, Cplx SQA）**:
   - 这个数据集包括更复杂的问题（包括计数、算术、比较和无法回答的问题），并且包含具有各种纵横比的屏幕。它旨在补充ScreenQA Short，通过引入更复杂的任务来挑战模型的能力。

这些数据集的发布，不仅使研究社区能够利用ScreenAI的文本表示进行研究，而且促进了对基于屏幕的问题回答模型进行更全面的基准测试。这些数据集的详细信息和访问方式在论文的附录部分以及相关的在线存储库中进行了描述。通过这些数据集，研究者可以评估模型在不同方面的性能，包括对象检测、文本生成和多模态理解等。
